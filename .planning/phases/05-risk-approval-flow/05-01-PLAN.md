---
phase: 05-risk-approval-flow
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/src/db/schema.ts
  - backend/src/config/env.ts
  - backend/src/services/approval.service.ts
  - backend/src/services/risk.service.ts
autonomous: true

must_haves:
  truths:
    - "approval_queue table exists in database with correct columns and indexes"
    - "LLM env vars (LLM_BASE_URL, LLM_API_KEY, LLM_MODEL, etc.) are validated at startup"
    - "Approval service can create, read, and transition approval queue entries"
    - "Risk service calls OpenAI-compatible LLM API with method heuristics and fail-closed behavior"
  artifacts:
    - path: "backend/src/db/schema.ts"
      provides: "approvalQueue table definition with 5-state machine"
      contains: "approvalQueue"
    - path: "backend/src/config/env.ts"
      provides: "LLM and risk threshold env var validation"
      contains: "LLM_BASE_URL"
    - path: "backend/src/services/approval.service.ts"
      provides: "Approval queue CRUD and state transitions"
      exports: ["createApprovalQueueEntry", "getApprovalQueueEntry", "expireStaleApprovals", "transitionStatus"]
    - path: "backend/src/services/risk.service.ts"
      provides: "LLM risk assessment with method heuristics"
      exports: ["assessRisk", "RiskResult"]
  key_links:
    - from: "backend/src/services/approval.service.ts"
      to: "backend/src/db/schema.ts"
      via: "drizzle query on approvalQueue"
      pattern: "approvalQueue"
    - from: "backend/src/services/risk.service.ts"
      to: "backend/src/config/env.ts"
      via: "env.LLM_BASE_URL in fetch call"
      pattern: "env\\.LLM_"
---

<objective>
Create the approval queue database table, LLM environment configuration, and two new services: approval.service.ts (queue CRUD + state machine) and risk.service.ts (LLM call + method heuristics + fail-closed).

Purpose: These are the foundational building blocks that Plan 02 will wire into the proxy flow and new routes.
Output: New DB migration, two new service files, updated env.ts and schema.ts.
</objective>

<execution_context>
@/home/shashank/.claude/get-shit-done/workflows/execute-plan.md
@/home/shashank/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-risk-approval-flow/05-RESEARCH.md
@backend/src/db/schema.ts
@backend/src/config/env.ts
@backend/src/services/proxy.service.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Approval queue schema, env vars, and approval service</name>
  <files>backend/src/db/schema.ts, backend/src/config/env.ts, backend/src/services/approval.service.ts</files>
  <action>
**1. Update backend/src/db/schema.ts — add approvalQueue table:**

Add the `approvalQueue` table following the existing pattern (pgTable, integer primary key, generatedAlwaysAsIdentity). Columns:
- `id`: integer().primaryKey().generatedAlwaysAsIdentity()
- `actionId`: varchar({ length: 36 }).notNull().unique() — UUID v4, generated in app code via crypto.randomUUID() (do NOT use uuid() Drizzle type to avoid pgcrypto dependency per research pitfall #7)
- `agentId`: integer().references(() => agents.id, { onDelete: 'cascade' }).notNull()
- `serviceId`: integer().references(() => services.id, { onDelete: 'cascade' }).notNull()
- `method`: varchar({ length: 10 }).notNull()
- `targetUrl`: varchar({ length: 2048 }).notNull()
- `requestHeaders`: text() — JSON-serialized, auth headers stripped before storage
- `requestBody`: text() — nullable
- `intent`: varchar({ length: 500 }).notNull()
- `riskScore`: real().notNull() — use `real` from drizzle-orm/pg-core (maps to PostgreSQL REAL, 4-byte float; sufficient for 0-1 scores)
- `riskExplanation`: text().notNull()
- `status`: varchar({ length: 20 }).notNull().default('PENDING') — values: PENDING, APPROVED, DENIED, EXPIRED, EXECUTED
- `approvalExpiresAt`: timestamp() — set when status flips to APPROVED
- `createdAt`: timestamp().defaultNow().notNull()
- `resolvedAt`: timestamp() — when APPROVED/DENIED set
- `executedAt`: timestamp() — when EXECUTED set
- `responseStatus`: integer() — cached execution result
- `responseHeaders`: text() — JSON-serialized cached result
- `responseBody`: text() — cached execution result

Indexes (in the table's third argument function):
- `actionIdIdx`: uniqueIndex('approval_queue_action_id_idx').on(table.actionId)
- `agentIdIdx`: index('approval_queue_agent_id_idx').on(table.agentId)
- `statusIdx`: index('approval_queue_status_idx').on(table.status)
- `createdAtIdx`: index('approval_queue_created_at_idx').on(table.createdAt)

Import `real` from 'drizzle-orm/pg-core' (add to existing import). Export types: `ApprovalQueueEntry` (InferSelectModel) and `InsertApprovalQueueEntry` (InferInsertModel).

**2. Update backend/src/config/env.ts — add LLM + risk env vars:**

Add to the `env` object:
- `LLM_BASE_URL`: getEnvVar('LLM_BASE_URL') — required
- `LLM_API_KEY`: getEnvVar('LLM_API_KEY') — required
- `LLM_MODEL`: getEnvVar('LLM_MODEL', false) || 'gpt-4o-mini' — optional with default
- `LLM_TIMEOUT_MS`: getEnvNumber('LLM_TIMEOUT_MS', 10000) — default 10s
- `RISK_THRESHOLD`: (() => { const val = parseFloat(process.env.RISK_THRESHOLD || '0.5'); if (isNaN(val) || val < 0 || val > 1) throw new Error('RISK_THRESHOLD must be 0-1'); return val; })()
- `APPROVAL_EXECUTE_TTL_HOURS`: getEnvNumber('APPROVAL_EXECUTE_TTL_HOURS', 1) — default 1 hour

**3. Run database migration:**

Run `cd backend && bun run db:generate && bun run db:migrate` to create the new table.

NOTE: Check the generated migration SQL. If it tries to recreate existing tables (Drizzle regenerates full schema), you may need to manually edit the migration to only include the approval_queue CREATE TABLE. Follow the pattern documented in Phase 04 decisions.

**4. Create backend/src/services/approval.service.ts:**

Exports:
- `createApprovalQueueEntry(params)`: Takes { agentId, serviceId, method, targetUrl, requestHeaders (already stripped of auth), requestBody, intent, riskScore, riskExplanation }. Generates actionId via crypto.randomUUID(). Inserts into approvalQueue with status 'PENDING'. Returns the actionId string.
- `getApprovalQueueEntry(actionId: string)`: SELECT from approvalQueue WHERE actionId = param. Returns row or null.
- `transitionStatus(actionId: string, fromStatus: string, toStatus: string, extras?: Partial<...>)`: Conditional UPDATE WHERE actionId AND status = fromStatus. Sets status to toStatus plus any extras (resolvedAt, executedAt, approvalExpiresAt, response fields). Returns boolean indicating if the update matched (rowCount > 0). This is the race-safe pattern from research pitfall #3.
- `expireStaleApprovals()`: UPDATE approvalQueue SET status='EXPIRED' WHERE status='APPROVED' AND approvalExpiresAt < NOW(). Uses conditional WHERE for race safety.
- `markExecuted(actionId: string, responseStatus: number, responseHeaders: string, responseBody: string)`: Calls transitionStatus('APPROVED', 'EXECUTED') with executedAt=new Date() and the response fields.

Use imports: `db` from '@/config/db', `approvalQueue` from '@/db/schema', `eq`, `and`, `lt` from 'drizzle-orm'.
  </action>
  <verify>
Run `cd /home/shashank/personal/gaiter-gaurd/backend && bun run db:generate && bun run db:migrate` succeeds. Then verify: `bun -e "import { approvalQueue } from './src/db/schema'; console.log(Object.keys(approvalQueue))"` prints table columns. `bun -e "import { env } from './src/config/env'; console.log(env.LLM_BASE_URL, env.RISK_THRESHOLD)"` prints env values (requires .env to have LLM_BASE_URL and LLM_API_KEY set). `bun -e "import * as svc from './src/services/approval.service'; console.log(typeof svc.createApprovalQueueEntry, typeof svc.getApprovalQueueEntry, typeof svc.transitionStatus, typeof svc.expireStaleApprovals)"` prints "function function function function".
  </verify>
  <done>approvalQueue table exists in DB with all columns and indexes. env.ts validates LLM_BASE_URL, LLM_API_KEY, LLM_MODEL, LLM_TIMEOUT_MS, RISK_THRESHOLD, APPROVAL_EXECUTE_TTL_HOURS. approval.service.ts exports createApprovalQueueEntry, getApprovalQueueEntry, transitionStatus, expireStaleApprovals, markExecuted.</done>
</task>

<task type="auto">
  <name>Task 2: Risk assessment service with LLM call and method heuristics</name>
  <files>backend/src/services/risk.service.ts</files>
  <action>
Create backend/src/services/risk.service.ts with the following exports:

**Types:**
```typescript
export interface RiskInput {
  intent: string;
  method: string;
  targetUrl: string;
  body: string | null;
}

export interface RiskResult {
  score: number;       // 0-1
  explanation: string; // human-readable
  blocked: boolean;    // true if score >= threshold
}
```

**methodBaseScore(method: string): number** (not exported, internal helper)
Returns baseline risk score by HTTP method per user decision (method heuristics):
- DELETE: 0.7
- PUT: 0.5
- PATCH: 0.4
- POST: 0.3
- GET: 0.1
- HEAD/OPTIONS: 0.05
- default: 0.2

**RISK_SYSTEM_PROMPT** (const, not exported)
Use the exact prompt from research Pattern 2 code example. MUST include "You must respond with valid JSON" instruction (per research pitfall #5 — OpenAI JSON mode requires this). The prompt instructs the model to output `{"score": <number 0.0-1.0>, "explanation": "<one sentence>"}` with score guidelines.

**buildRiskUserPrompt(intent, method, targetUrl, body)** (internal helper)
Constructs user message showing agent's stated intent vs actual HTTP request. Truncate body to first 500 chars to keep tokens low.

**callLLMForRiskAssessment(intent, method, targetUrl, body): Promise<{ score: number; explanation: string }>**
- Uses `fetch()` to call `${env.LLM_BASE_URL}/chat/completions` with:
  - method: POST
  - headers: Content-Type application/json, Authorization Bearer ${env.LLM_API_KEY}
  - body: JSON with model: env.LLM_MODEL, response_format: { type: 'json_object' }, temperature: 0, max_tokens: 300, messages array with system + user prompts
- AbortController with timeout = env.LLM_TIMEOUT_MS (default 10s, separate from proxy 30s timeout per research pitfall #2)
- On success: parse JSON from choices[0].message.content, validate score is number 0-1 and explanation is string
- On ANY failure (timeout, non-200 status, invalid JSON, missing fields): throw error (caller handles fail-closed)
- Clamp score to [0, 1] with Math.max(0, Math.min(1, score))

**assessRisk(params: RiskInput): Promise<RiskResult>** (main export)
Per user decision: LLM evaluates using intent mismatch detection AND HTTP method heuristics.
- Get heuristicScore from methodBaseScore
- Try calling callLLMForRiskAssessment
  - On success: blend scores — `llmScore * 0.7 + heuristicScore * 0.3` (LLM opinion weighted higher)
  - On failure (catch block): FAIL CLOSED per user decision — use `Math.min(1, heuristicScore + 0.3)` as score, explanation = "Risk assessed via method heuristics only (LLM unavailable). Method: {method}"
- Compare final score against env.RISK_THRESHOLD
- Return { score, explanation, blocked: score >= env.RISK_THRESHOLD }

Import `env` from '@/config/env'.
  </action>
  <verify>
Run `bun -e "import { assessRisk } from './src/services/risk.service'; console.log(typeof assessRisk)"` from backend/ prints "function". TypeScript compilation: `cd /home/shashank/personal/gaiter-gaurd/backend && bunx tsc --noEmit src/services/risk.service.ts` has no errors.
  </verify>
  <done>risk.service.ts exports assessRisk and RiskResult. LLM call uses OpenAI-compatible format with AbortController timeout. Method heuristics provide baseline scores. Fail-closed on any LLM error (timeout, bad response, invalid JSON). Score blending weights LLM at 0.7 and heuristic at 0.3.</done>
</task>

</tasks>

<verification>
- `approval_queue` table exists in PostgreSQL with all columns, indexes, and correct types
- env.ts validates all 6 new env vars without crashing at import time
- approval.service.ts compiles and exports all CRUD + state transition functions
- risk.service.ts compiles and exports assessRisk with correct RiskResult type
- No changes to existing proxy.service.ts or routes (that's Plan 02)
</verification>

<success_criteria>
- New approval_queue DB table with 5-state machine (PENDING/APPROVED/DENIED/EXPIRED/EXECUTED)
- LLM env vars validated at startup with sensible defaults
- Approval service handles create, read, conditional state transitions, and TTL expiry
- Risk service combines LLM intent analysis with HTTP method heuristics
- Fail-closed behavior: LLM errors result in blocked=true
- All code compiles without TypeScript errors
</success_criteria>

<output>
After completion, create `.planning/phases/05-risk-approval-flow/05-01-SUMMARY.md`
</output>
